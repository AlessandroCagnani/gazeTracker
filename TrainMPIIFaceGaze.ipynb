{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyPz9bisGuxN2PPoaotbHMmC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install kaggle\n","!mkdir ~/.kaggle\n","!cp kaggle.json ~/.kaggle/\n","!chmod 600 ~/.kaggle/kaggle.json\n","\n","!kaggle datasets download squeakink/mpiifacegaze\n"],"metadata":{"id":"MLq1gZa7b5ch"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip mpiifacegaze"],"metadata":{"id":"bvdFe0c9ckdY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import numpy as np\n","import random\n","import torch\n","import os\n","\n","seed = 777\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","\n","torch.backends.cudnn.benchmark = True\n","torch.backends.cudnn.deterministic = False\n","\n","batch_size = 32\n","optimizer = 'sgd'\n","epochs = 15\n","test_id = 0\n","num_workers = 2\n","drop_last = True\n","pin_memory = False\n","\n","if torch.cuda.is_available():\n","    available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n","    print(available_gpus)"],"metadata":{"id":"NytKO6oKq8rz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Callable, Tuple\n","import h5py\n","from torch.utils.data import Dataset\n","\n","\n","class OnePersonDataset(Dataset):\n","    def __init__(self, person_id_str, dataset_path,\n","                 transform: Callable):\n","        self.person_id_str = person_id_str\n","        self.dataset_path = dataset_path\n","        self.transform = transform\n","\n","    def __getitem__(\n","            self,\n","            index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","        with h5py.File(self.dataset_path, 'r') as f:\n","            image = f.get(f'{self.person_id_str}/image/{index:04}')[()]\n","            pose = f.get(f'{self.person_id_str}/pose/{index:04}')[()]\n","            gaze = f.get(f'{self.person_id_str}/gaze/{index:04}')[()]\n","        image = self.transform(image)\n","        pose = torch.from_numpy(pose)\n","        gaze = torch.from_numpy(gaze)\n","        return image, pose, gaze\n","\n","    def __len__(self) -> int:\n","        return 3000"],"metadata":{"id":"wk6hRjWpjIj6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Any\n","\n","import cv2\n","import numpy as np\n","import torch\n","import torchvision\n","\n","def trans(x):\n","    return x.transpose(2, 0, 1)\n","\n","def scale(x):\n","    return x.astype(np.float32) / 255\n","\n","def _create_mpiifacegaze_transform() -> Any:\n","    transform = torchvision.transforms.Compose([\n","        trans,\n","        scale,\n","        torch.from_numpy,\n","        torchvision.transforms.Normalize(mean=[0.406, 0.456, 0.485],\n","                                         std=[0.225, 0.224, 0.229]),\n","    ])\n","    return transform"],"metadata":{"id":"2VSFjIT4jMCk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Tuple, Union\n","from torch.utils.data import DataLoader\n","\n","#dataset_dir = '/kaggle/input/mpiifacegaze/MPIIFaceGaze.h5'\n","dataset_dir = '/content/MPIIFaceGaze.h5'\n","\n","val_ratio =  0.1\n","\n","\n","def create_dataset():\n","    person_ids = [f'p{index:02}' for index in range(15)]\n","\n","    transform = _create_mpiifacegaze_transform()\n","    if test_id == -1:\n","        train_dataset = torch.utils.data.ConcatDataset([\n","            OnePersonDataset(person_id, dataset_dir, transform)\n","            for person_id in person_ids\n","        ])\n","        assert len(train_dataset) == 45000\n","    else:\n","        test_person_id = person_ids[test_id]\n","        train_dataset = torch.utils.data.ConcatDataset([\n","            OnePersonDataset(person_id, dataset_dir, transform)\n","            for person_id in person_ids if person_id != test_person_id\n","        ])\n","        assert len(train_dataset) == 42000\n","\n","    assert val_ratio < 1\n","    val_num = int(len(train_dataset) * val_ratio)\n","    train_num = len(train_dataset) - val_num\n","    lengths = [train_num, val_num]\n","    return torch.utils.data.dataset.random_split(train_dataset, lengths)\n","\n","def create_dataloader() -> Union[Tuple[DataLoader, DataLoader], DataLoader]:\n","    train_dataset, val_dataset = create_dataset()\n","    # print(train_dataset.__getitem__(0)[0].shape)\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=2,\n","        pin_memory=pin_memory,\n","        drop_last=drop_last,\n","    )\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        pin_memory=pin_memory,\n","        drop_last=drop_last,\n","    )\n","    return train_loader, val_loader\n","\n","train_loader, val_loader = create_dataloader()\n","test_loader = OnePersonDataset('p00', dataset_dir, _create_mpiifacegaze_transform())\n"],"metadata":{"id":"jnQ1b5bzjPZA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","train_entries = len(train_loader.dataset)\n","val_entries = len(val_loader.dataset)\n","test_entries = len(test_loader)\n","\n","print(\"Number of entries in the train loader:\", train_entries)\n","print(\"Number of entries in the val loader:\", val_entries)\n","print(\"Number of entries in the test loader:\", test_entries)\n","print(\"tot: \", train_entries + val_entries + test_entries)"],"metadata":{"id":"BbjX0YjKd6v-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Tuple, Union\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","\n","import cv2\n","\n","\n","\n","class alexnet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.feature_extractor = torchvision.models.alexnet(\n","            weights=True).features\n","        # While the pretrained models of torchvision are trained using\n","        # images with RGB channel order, in this repository images are\n","        # treated as BGR channel order.\n","        # Therefore, reverse the channel order of the first convolutional\n","        # layer.\n","        module = getattr(self.feature_extractor, '0')\n","        module.weight.data = module.weight.data[:, [2, 1, 0]]\n","\n","        self.conv1 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n","        self.conv2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n","        self.conv3 = nn.Conv2d(256, 1, kernel_size=1, stride=1, padding=0)\n","\n","        self.fc1 = nn.Linear(256 * 13**2, 4096)\n","        self.fc2 = nn.Linear(4096, 4096)\n","        self.fc3 = nn.Linear(4096, 2)\n","\n","        self._register_hook()\n","        self._initialize_weight()\n","\n","    def _initialize_weight(self) -> None:\n","        nn.init.normal_(self.conv1.weight, mean=0, std=0.01)\n","        nn.init.normal_(self.conv2.weight, mean=0, std=0.01)\n","        nn.init.normal_(self.conv3.weight, mean=0, std=0.001)\n","        nn.init.constant_(self.conv1.bias, val=0.1)\n","        nn.init.constant_(self.conv2.bias, val=0.1)\n","        nn.init.constant_(self.conv3.bias, val=1)\n","        nn.init.normal_(self.fc1.weight, mean=0, std=0.005)\n","        nn.init.normal_(self.fc2.weight, mean=0, std=0.0001)\n","        nn.init.normal_(self.fc3.weight, mean=0, std=0.0001)\n","        nn.init.constant_(self.fc1.bias, val=1)\n","        nn.init.zeros_(self.fc2.bias)\n","        nn.init.zeros_(self.fc3.bias)\n","\n","    def _register_hook(self) -> None:\n","        n_channels = self.conv1.in_channels\n","\n","        def hook(\n","            module: nn.Module, grad_in: Union[Tuple[torch.Tensor, ...],\n","                                              torch.Tensor],\n","            grad_out: Union[Tuple[torch.Tensor, ...], torch.Tensor]\n","        ) -> Optional[torch.Tensor]:\n","            return tuple(grad / n_channels for grad in grad_in)\n","\n","        self.conv3.register_backward_hook(hook)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        img = x[0].cpu().detach().numpy()\n","        img = np.transpose(img, (1, 2, 0))\n","        x = self.feature_extractor(x)\n","        y = F.relu(self.conv1(x))\n","        y = F.relu(self.conv2(y))\n","        y = F.relu(self.conv3(y))\n","        test = y[0].cpu().detach().numpy()\n","        reshaped_vector = np.reshape(test, (13, 13))\n","        plt.figure()\n","        plt.subplot(1, 2, 1)\n","        plt.imshow(reshaped_vector)\n","        plt.subplot(1, 2, 2)\n","        plt.imshow(img)\n","        x = x * y\n","        x = x.view(x.size(0), -1)\n","        x = F.dropout(F.relu(self.fc1(x)), p=0.5, training=self.training)\n","        x = F.dropout(F.relu(self.fc2(x)), p=0.5, training=self.training)\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"LUlwnXL8jXAj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import Optional, Tuple, Union\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","import torch\n","import torchvision\n","\n","\n","class res_net_backbone(torchvision.models.ResNet):\n","    def __init__(self):\n","        block = torchvision.models.resnet.BasicBlock\n","        layers = [2, 2, 2] + [1]\n","        super().__init__(block, layers)\n","        del self.layer4\n","        del self.avgpool\n","        del self.fc\n","\n","        pretrained_name = \"resnet18\"\n","        if pretrained_name:\n","            # Load the appropriate ResNet model using the available functions\n","            pretrained_model = getattr(torchvision.models, pretrained_name)(pretrained=True)\n","            # Get the state dict from the loaded pretrained model\n","            state_dict = pretrained_model.state_dict()\n","\n","            self.load_state_dict(state_dict, strict=False)\n","            # While the pretrained models of torchvision are trained\n","            # using images with RGB channel order, in this repository\n","            # images are treated as BGR channel order.\n","            # Therefore, reverse the channel order of the first\n","            # convolutional layer.\n","            module = self.conv1\n","            module.weight.data = module.weight.data[:, [2, 1, 0]]\n","\n","        with torch.no_grad():\n","            data = torch.zeros((1, 3, 224, 224), dtype=torch.float32)\n","            features = self.forward(data)\n","            self.n_features = features.shape[1]\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        return x\n","\n","\n","class resNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.feature_extractor = res_net_backbone()\n","        n_channels = self.feature_extractor.n_features\n","\n","        self.conv = nn.Conv2d(n_channels,\n","                              1,\n","                              kernel_size=1,\n","                              stride=1,\n","                              padding=0)\n","        # This model assumes the input image size is 224x224.\n","        self.fc = nn.Linear(n_channels * 14**2, 2)\n","\n","        self._register_hook()\n","        self._initialize_weight()\n","\n","    def _initialize_weight(self) -> None:\n","        nn.init.kaiming_normal_(self.conv.weight)\n","        nn.init.zeros_(self.conv.bias)\n","        nn.init.xavier_normal_(self.fc.weight)\n","        nn.init.zeros_(self.fc.bias)\n","\n","    def _register_hook(self):\n","        n_channels = self.feature_extractor.n_features\n","\n","        def hook(\n","            module: nn.Module, grad_in: Union[Tuple[torch.Tensor, ...],\n","                                              torch.Tensor],\n","            grad_out: Union[Tuple[torch.Tensor, ...], torch.Tensor]\n","        ) -> Optional[torch.Tensor]:\n","            return tuple(grad / n_channels for grad in grad_in)\n","\n","        self.conv.register_backward_hook(hook)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.feature_extractor(x)\n","        y = F.relu(self.conv(x))\n","        x = x * y\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n"],"metadata":{"id":"7entsQq69F8v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install fvcore"],"metadata":{"id":"TqNugyO9jbRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","from typing import Tuple\n","import pathlib\n","import numpy as np\n","import torch\n","\n","\n","def setup_cudnn(config) -> None:\n","    torch.backends.cudnn.benchmark = config.cudnn.benchmark\n","    torch.backends.cudnn.deterministic = config.cudnn.deterministic\n","\n","def create_train_output_dir(config):\n","    output_root_dir = pathlib.Path(config.train.output_dir)\n","    if config.train.test_id != -1:\n","        output_dir = output_root_dir / f'{config.train.test_id:02}'\n","    else:\n","        output_dir = output_root_dir / 'all'\n","    if output_dir.exists():\n","        raise RuntimeError(\n","            f'Output directory `{output_dir.as_posix()}` already exists.')\n","    output_dir.mkdir(exist_ok=True, parents=True)\n","    return output_dir\n","\n","\n","def convert_to_unit_vector(\n","        angles: torch.Tensor\n",") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","    pitches = angles[:, 0]\n","    yaws = angles[:, 1]\n","    x = -torch.cos(pitches) * torch.sin(yaws)\n","    y = -torch.sin(pitches)\n","    z = -torch.cos(pitches) * torch.cos(yaws)\n","    norm = torch.sqrt(x**2 + y**2 + z**2)\n","    x /= norm\n","    y /= norm\n","    z /= norm\n","    return x, y, z\n","\n","\n","def compute_angle_error(predictions: torch.Tensor,\n","                        labels: torch.Tensor) -> torch.Tensor:\n","    pred_x, pred_y, pred_z = convert_to_unit_vector(predictions)\n","    label_x, label_y, label_z = convert_to_unit_vector(labels)\n","    angles = pred_x * label_x + pred_y * label_y + pred_z * label_z\n","    return torch.acos(angles) * 180 / np.pi\n","\n","\n","class AverageMeter:\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, num):\n","        self.val = val\n","        self.sum += val * num\n","        self.count += num\n","        self.avg = self.sum / self.count"],"metadata":{"id":"gFgfCFZcjeV8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","def train(epoch, model, optimizer, scheduler, loss_function, train_loader, tensorboard_writer):\n","    print(f'Train {epoch}')\n","\n","    model.train()\n","\n","    device = torch.device(_device)\n","\n","    loss_meter = AverageMeter()\n","    angle_error_meter = AverageMeter()\n","    start = time.time()\n","\n","    for step, (images, poses, gazes) in enumerate(train_loader):\n","        images = images.to(device)\n","        poses = poses.to(device)\n","        gazes = gazes.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(images)\n","        loss = loss_function(outputs, gazes)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        angle_error = compute_angle_error(outputs, gazes).mean()\n","\n","        num = images.size(0)\n","        loss_meter.update(loss.item(), num)\n","        angle_error_meter.update(angle_error.item(), num)\n","\n","        if step % 100 == 0:\n","            print(f'Epoch {epoch} \\n',\n","                  f'Step {step}/{len(train_loader)} \\n',\n","                  f'lr {scheduler.get_last_lr()[0]:.6f} \\n',\n","                  f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f}) \\n',\n","                  f'angle error {angle_error_meter.val:.2f} ',\n","                  f'({angle_error_meter.avg:.2f})\\n')\n","\n","    elapsed = time.time() - start\n","    print(f'Elapsed {elapsed:.2f}')\n","\n","    tensorboard_writer.add_scalar('Train/Loss', loss_meter.avg, epoch)\n","    tensorboard_writer.add_scalar('Train/lr', scheduler.get_last_lr()[0], epoch)\n","    tensorboard_writer.add_scalar('Train/AngleError', angle_error_meter.avg, epoch)\n","    tensorboard_writer.add_scalar('Train/Time', elapsed, epoch)\n","\n","    return loss_meter.avg, angle_error_meter.avg\n","\n","\n","\n","def validate(epoch, model, loss_function, val_loader,\n","             tensorboard_writer):\n","    print(f'Val {epoch}')\n","\n","    model.eval()\n","\n","    device = torch.device(_device)\n","\n","    loss_meter = AverageMeter()\n","    angle_error_meter = AverageMeter()\n","    start = time.time()\n","\n","    with torch.no_grad():\n","        for step, (images, poses, gazes) in enumerate(val_loader):\n","\n","            images = images.to(device)\n","            poses = poses.to(device)\n","            gazes = gazes.to(device)\n","\n","            outputs = model(images)\n","\n","            loss = loss_function(outputs, gazes)\n","\n","            angle_error = compute_angle_error(outputs, gazes).mean()\n","\n","            num = images.size(0)\n","            loss_meter.update(loss.item(), num)\n","            angle_error_meter.update(angle_error.item(), num)\n","\n","    print(f'Epoch {epoch} \\n',\n","          f'loss {loss_meter.avg:.4f} \\n'\n","          f'angle error {angle_error_meter.avg:.2f}')\n","\n","    elapsed = time.time() - start\n","    print(f'Elapsed {elapsed:.2f}')\n","\n","    if epoch > 0:\n","        tensorboard_writer.add_scalar('Val/Loss', loss_meter.avg, epoch)\n","        tensorboard_writer.add_scalar('Val/AngleError', angle_error_meter.avg,\n","                                      epoch)\n","    tensorboard_writer.add_scalar('Val/Time', elapsed, epoch)\n","\n","    return loss_meter.avg, angle_error_meter.avg"],"metadata":{"id":"fLv-qha5jjaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorboardX"],"metadata":{"id":"Zw4alF7Umlbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from fvcore.common.checkpoint import Checkpointer\n","from tensorboardX import SummaryWriter\n","\n","model = alexnet()\n","\n","_device = 'cuda:0'\n","\n","device = torch.device(_device)\n","model.to(device)\n","\n","loss_fc = nn.L1Loss(reduction='mean')\n","\n","#output_dir = '/kaggle/working/results/'\n","output_dir = './'\n","params = [{'params': list(model.parameters()), 'weight_decay': 0.0001,}]\n","\n","optimizer = torch.optim.SGD(params,\n","                            lr=0.01,\n","                            momentum=0.9,\n","                            nesterov=True)\n","\n","scheduler = torch.optim.lr_scheduler.MultiStepLR(\n","            optimizer,\n","            milestones=[10, 13],\n","            gamma=0.1)\n","\n","checkpointer = Checkpointer(model,\n","                            optimizer=optimizer,\n","                            scheduler=scheduler,\n","                            save_dir=output_dir,\n","                            save_to_disk=True)\n","\n","tensorboard_writer = SummaryWriter(output_dir)\n","\n","validate(0, model, loss_fc, val_loader,\n","                 tensorboard_writer)\n","\n","\n","train_losses = []\n","val_losses = []\n","train_accuracies = []\n","val_accuracies = []\n","\n","for epoch in range(1, epochs + 1):\n","    train_loss, train_accuracy = train(epoch, model, optimizer, scheduler, loss_fc, train_loader, tensorboard_writer)\n","    scheduler.step()\n","\n","    train_losses.append(train_loss)\n","    train_accuracies.append(train_accuracy)\n","\n","    if epoch % 1 == 0:\n","        val_loss, val_accuracy = validate(epoch, model, loss_fc, val_loader, tensorboard_writer)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_accuracy)\n","\n","    if (epoch % 5 == 0 or epoch == epochs):\n","        config = {\n","            'batch_size': 32,\n","            'optimizer': 'sgd',\n","            'epochs': 15,\n","            'test_id': 0,\n","            'num_workers': 2,\n","            'drop_last': True,\n","            'pin_memory': False\n","        }\n","        checkpoint_config = {'epoch': epoch, 'config': config}\n","        checkpointer.save(f'checkpoint_{epoch:04d}', **checkpoint_config)\n","\n","tensorboard_writer.close()"],"metadata":{"id":"pCJe-8G7mhaj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(range(1, epochs+1), train_losses, label='Train loss')\n","plt.plot(range(1, epochs+1), val_losses, label='Val loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.title('Loss over time')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(range(1, epochs+1), train_accuracies, label='Train accuracy')\n","plt.plot(range(1, epochs+1), val_accuracies, label='Val accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Angle Error')\n","plt.title('Angle Error over time')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"MWyFI32f-WYB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","model = alexnet()\n","checkpoint = torch.load(\"checkpoint_0015.pth\",  map_location='cuda')\n","model.load_state_dict(checkpoint['model'])\n","model.to(torch.device('cuda'))\n","loss_fc = nn.L1Loss(reduction='mean')\n","\n","def evaluate(model, dataloader, criterion, device):\n","    model.eval()  # set the model to evaluation mode\n","    total_loss = 0\n","    total_correct = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():  # disable gradient calculation\n","        for step, (images, poses, gazes) in enumerate(dataloader):\n","            inputs = images.to(device)\n","            targets = gazes.to(device)\n","            print(images.shape)\n","            outputs = model(inputs)\n","            loss = nn.L1loss(outputs, targets)\n","\n","            _, predicted = torch.max(outputs, 1)\n","            correct = (predicted == targets).sum().item()\n","            samples = targets.size(0)\n","\n","            total_loss += loss.item() * samples\n","            total_correct += correct\n","            total_samples += samples\n","\n","    avg_loss = total_loss / total_samples\n","    accuracy = total_correct / total_samples\n","\n","    return avg_loss, accuracy\n","\n","\n","test_loss, test_accuracy = evaluate(model, test_loader, loss_fc, 'cuda')\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"],"metadata":{"id":"bnr7iYs653kB"},"execution_count":null,"outputs":[]}]}